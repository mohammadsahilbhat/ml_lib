{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486018d9",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29ca8",
   "metadata": {},
   "source": [
    "\n",
    "## Using Logistic Regression\n",
    "- Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8387b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ml_lib.linear_models.logistic_regression import LogisticRegression\n",
    "from ml_lib.metrics.math import accuracy\n",
    "from ml_lib.preprocessing.scaler import StandardScaler\n",
    "from ml_lib.utils.data import train_test_split\n",
    "from ml_lib.preprocessing.pipeline import Pipeline\n",
    "from ml_lib.preprocessing.imputer import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221d901",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0f1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 40 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   feature_0   25000 non-null  float64\n",
      " 1   feature_1   25000 non-null  float64\n",
      " 2   feature_2   25000 non-null  float64\n",
      " 3   feature_3   25000 non-null  float64\n",
      " 4   feature_4   25000 non-null  float64\n",
      " 5   feature_5   25000 non-null  float64\n",
      " 6   feature_6   25000 non-null  float64\n",
      " 7   feature_7   25000 non-null  float64\n",
      " 8   feature_8   25000 non-null  float64\n",
      " 9   feature_9   25000 non-null  float64\n",
      " 10  feature_10  25000 non-null  float64\n",
      " 11  feature_11  25000 non-null  float64\n",
      " 12  feature_12  25000 non-null  float64\n",
      " 13  feature_13  25000 non-null  float64\n",
      " 14  feature_14  25000 non-null  float64\n",
      " 15  feature_15  25000 non-null  float64\n",
      " 16  feature_16  25000 non-null  float64\n",
      " 17  feature_17  25000 non-null  float64\n",
      " 18  feature_18  25000 non-null  float64\n",
      " 19  feature_19  25000 non-null  float64\n",
      " 20  feature_20  25000 non-null  float64\n",
      " 21  feature_21  25000 non-null  float64\n",
      " 22  feature_22  25000 non-null  float64\n",
      " 23  feature_23  25000 non-null  float64\n",
      " 24  feature_24  25000 non-null  float64\n",
      " 25  feature_25  25000 non-null  float64\n",
      " 26  feature_26  25000 non-null  float64\n",
      " 27  feature_27  25000 non-null  float64\n",
      " 28  feature_28  25000 non-null  float64\n",
      " 29  feature_29  25000 non-null  float64\n",
      " 30  feature_30  25000 non-null  float64\n",
      " 31  feature_31  25000 non-null  float64\n",
      " 32  feature_32  25000 non-null  float64\n",
      " 33  feature_33  25000 non-null  float64\n",
      " 34  feature_34  25000 non-null  float64\n",
      " 35  feature_35  25000 non-null  float64\n",
      " 36  feature_36  25000 non-null  float64\n",
      " 37  feature_37  25000 non-null  float64\n",
      " 38  feature_38  25000 non-null  float64\n",
      " 39  feature_39  25000 non-null  float64\n",
      "dtypes: float64(40)\n",
      "memory usage: 7.6 MB\n",
      "Original rows: 50001\n",
      "Rows kept: 49999\n",
      "Rows removed: 2\n",
      "Shape of X : (49999, 40)\n",
      "Shape of y : (49999,)\n"
     ]
    }
   ],
   "source": [
    "train_df= pd.read_csv(r\"C:/project/datasets/train_multi_class.csv\")\n",
    "test_df= pd.read_csv(r\"C:/project/datasets/test_multi_class.csv\")\n",
    "\n",
    "test_df.info()\n",
    "train_df_clean=train_df.dropna(subset=[train_df.columns[-1]])\n",
    "\n",
    "print(\"Original rows:\", len(train_df))\n",
    "print(\"Rows kept:\", len(train_df_clean))\n",
    "print(\"Rows removed:\", len(train_df) - len(train_df_clean))\n",
    "\n",
    "\n",
    "\n",
    "X= train_df_clean.drop(\"target\",axis=1).values\n",
    "y=train_df_clean[\"target\"].values\n",
    "\n",
    "print(f\"Shape of X : {X.shape}\\nShape of y : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd4c11",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd49660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999, 40)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='mean')),\n",
    "    ('scaler',StandardScaler())\n",
    "    ])\n",
    "\n",
    "X_trans= pipe.fit(X)\n",
    "print(X_trans.shape)\n",
    "\n",
    "X_train,y_train,X_val,y_val= train_test_split(X_trans,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb89bc1",
   "metadata": {},
   "source": [
    "count unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08133cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the classes are [0. 1. 2. 3. 4.] and the no of classes are  5\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y)\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(\"the classes are\",classes ,\"and the no of classes are \", n_classes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dabbb",
   "metadata": {},
   "source": [
    "### Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea98126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "for cls in classes:\n",
    "    y_binary= (y_train==cls).astype(int)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        lr= 0.005,\n",
    "        epochs=5000\n",
    "    )\n",
    "    model.fit(X_train,y_binary)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b9c51",
   "metadata": {},
   "source": [
    "NOte  it takes much time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f9548",
   "metadata": {},
   "source": [
    "Predictions function for all classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e25073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_predict(X):\n",
    "    probs=[]\n",
    "    for model in models:\n",
    "        probs.append(model.predict_proba(X))\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    return np.argmax(probs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfb826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.5766576657665766\n"
     ]
    }
   ],
   "source": [
    "y_pred = multiclass_predict(X_val)\n",
    "acc=accuracy(y_val,y_pred)\n",
    "\n",
    "print(f\"Test accuracy : {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b4171",
   "metadata": {},
   "source": [
    "- as accuracy is very low so we use neural network method for the multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b8837",
   "metadata": {},
   "source": [
    "## Classification using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b428e6e",
   "metadata": {},
   "source": [
    "- IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fc1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_lib.neural_network.layers import Dense\n",
    "from ml_lib.neural_network.sequential import Sequential\n",
    "from ml_lib.neural_network.losses import CategoricalCrossEntropy\n",
    "from ml_lib.preprocessing.encoding import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a47cb",
   "metadata": {},
   "source": [
    "- Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ca9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: [0 1 2 3 4]\n",
      "One-hot shape: (49999, 5)\n"
     ]
    }
   ],
   "source": [
    "y= np.nan_to_num(y, nan=5)\n",
    "y = y.astype(int)\n",
    "\n",
    "le= LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y_train_oh = ohe.fit_transform(y_train_enc.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train labels:\", np.unique(y_train_enc))\n",
    "print(\"One-hot shape:\", y_train_oh.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998cb857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 11.0305\n",
      "Epoch 20 | Loss: 2.0839\n",
      "Epoch 40 | Loss: 1.1311\n",
      "Epoch 60 | Loss: 0.9295\n",
      "Epoch 80 | Loss: 0.8367\n",
      "Epoch 100 | Loss: 0.7785\n",
      "Epoch 120 | Loss: 0.7366\n",
      "Epoch 140 | Loss: 0.7056\n",
      "Epoch 160 | Loss: 0.6817\n",
      "Epoch 180 | Loss: 0.6619\n",
      "Epoch 200 | Loss: 0.6450\n",
      "Epoch 220 | Loss: 0.6302\n",
      "Epoch 240 | Loss: 0.6173\n",
      "Epoch 260 | Loss: 0.6059\n",
      "Epoch 280 | Loss: 0.5955\n",
      "Epoch 300 | Loss: 0.5860\n",
      "Epoch 320 | Loss: 0.5775\n",
      "Epoch 340 | Loss: 0.5703\n",
      "Epoch 360 | Loss: 0.5640\n",
      "Epoch 380 | Loss: 0.5583\n",
      "Epoch 400 | Loss: 0.5530\n",
      "Epoch 420 | Loss: 0.5482\n",
      "Epoch 440 | Loss: 0.5438\n",
      "Epoch 460 | Loss: 0.5397\n",
      "Epoch 480 | Loss: 0.5359\n",
      "Epoch 500 | Loss: 0.5324\n",
      "Epoch 520 | Loss: 0.5290\n",
      "Epoch 540 | Loss: 0.5258\n",
      "Epoch 560 | Loss: 0.5227\n",
      "Epoch 580 | Loss: 0.5199\n",
      "Epoch 600 | Loss: 0.5172\n",
      "Epoch 620 | Loss: 0.5147\n",
      "Epoch 640 | Loss: 0.5123\n",
      "Epoch 660 | Loss: 0.5100\n",
      "Epoch 680 | Loss: 0.5077\n",
      "Epoch 700 | Loss: 0.5055\n",
      "Epoch 720 | Loss: 0.5034\n",
      "Epoch 740 | Loss: 0.5013\n",
      "Epoch 760 | Loss: 0.4993\n",
      "Epoch 780 | Loss: 0.4973\n",
      "Epoch 800 | Loss: 0.4954\n",
      "Epoch 820 | Loss: 0.4934\n",
      "Epoch 840 | Loss: 0.4916\n",
      "Epoch 860 | Loss: 0.4898\n",
      "Epoch 880 | Loss: 0.4880\n",
      "Epoch 900 | Loss: 0.4863\n",
      "Epoch 920 | Loss: 0.4846\n",
      "Epoch 940 | Loss: 0.4830\n",
      "Epoch 960 | Loss: 0.4813\n",
      "Epoch 980 | Loss: 0.4797\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "(Dense(128, activation=\"relu\",init= \"he\")),\n",
    "(Dense(64, activation=\"relu\",init=\"he\")),\n",
    "(Dense(n_classes, activation=\"softmax\",init=\"xavier\"))])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss= CategoricalCrossEntropy(),lr=0.005)\n",
    "\n",
    "history = model.fit(X_trans, y_train_oh, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5d10e",
   "metadata": {},
   "source": [
    "- check the accuracy percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a56e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy :  85.06850685068507 %\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "y_val_labels=le.inverse_transform(y_val_pred)\n",
    "\n",
    "val_acc= accuracy(y_val,y_val_labels)*100\n",
    "print(f\"Validation Accuracy : \",val_acc,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f56430",
   "metadata": {},
   "source": [
    "for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e162d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 40) 25000\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = train_df.columns[:-1] \n",
    "X_test = test_df[feature_cols].values\n",
    "print(X_test.shape,X_test.shape[0])\n",
    "X_test_trans = pipe.transform(X_test)\n",
    "y_t_pred = model.predict(X_test_trans)\n",
    "y_test_pred=np.argmax(y_t_pred,axis=1)\n",
    "y_test_pred=le.inverse_transform(y_test_pred)\n",
    "print(y_test_pred.shape)\n",
    "id = np.arange(1,X_test_trans.shape[0]+1)\n",
    "df2=pd.DataFrame({\n",
    "    'id':id,\n",
    "    'target':y_test_pred\n",
    "})\n",
    "df2.to_csv(r\"C:/project/datasets/processed/multiclass_classification_NN.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
